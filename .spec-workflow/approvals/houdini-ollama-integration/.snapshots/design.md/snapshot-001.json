{
  "id": "snapshot_1758895050439_p093fkoj1",
  "approvalId": "approval_1758895050429_38ikq6ua5",
  "approvalTitle": "Approve design for Houdini Ollama integration",
  "version": 1,
  "timestamp": "2025-09-26T13:57:30.438Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Design Document\r\n\r\n## Overview\r\n\r\nImplement a reliable WebChannel and network integration layer for the Cherry Studio Houdini plugin. The design stabilizes three pillars: (1) deterministic WebChannel initialization between Qt and the embedded frontend, (2) a Python-side proxy that mediates Ollama network calls and returns structured responses, and (3) comprehensive diagnostics and fallbacks to eliminate Houdini crashes or silent failures.\r\n\r\n## Steering Document Alignment\r\n\r\n### Technical Standards (tech.md)\r\n- Follow existing PySide6/WebChannel usage patterns.\r\n- Centralize network access in Python to ensure consistent logging and exception handling.\r\n- Maintain backward compatibility with current Cherry Studio frontend expectations (window.api, window.qt).\r\n\r\n### Project Structure (structure.md)\r\n- Keep all Houdini-specific Python logic inside `houdini_plugin/main.py`.\r\n- Frontend adjustments limited to `web/src/renderer/.../OpenAIBaseClient.ts` and small utility stubs.\r\n- Inject helper scripts through `QWebEngineScript` while keeping the main Qt bootstrap clean and modular.\r\n\r\n## Code Reuse Analysis\r\n\r\n### Existing Components to Leverage\r\n- **Existing WebChannel registration logic**: Extend the current `create_window` function to include consistent ping and initialization hooks.\r\n- **NetworkAPI class**: Reuse and expand it to provide a dedicated `openaiFetch` route for Ollama.\r\n- **Ollama client integration**: Reuse the existing `OpenAIBaseClient` branch for Ollama with new fallbacks.\r\n\r\n### Integration Points\r\n- **window.api / window.qt API surface**: Maintain compatibility with the React frontend expecting Electron-like APIs.\r\n- **Frontend model management view**: The design needs to ensure model lists are fetched via the Qt proxy.\r\n- **Ollama local REST endpoints**: The proxy will invoke `/api/tags` (and, if needed, `/v1/models`) using Python’s `urllib`.\r\n\r\n## Architecture\r\n\r\n```mermaid\r\ngraph TD\r\n    A[Qt App Bootstrap] --> B[WebChannel Setup]\r\n    B --> C[Python NetworkAPI]\r\n    B --> D[AppAPI ping]\r\n    C --> E[Ollama REST]\r\n    D --> F[Frontend Diagnostics]\r\n    F --> B\r\n```\r\n\r\n### Modular Design Principles\r\n- **Single File Responsibility**: `main.py` handles Qt setup; frontend client files only handle translation and mapping.\r\n- **Component Isolation**: Separate classes for AppAPI, NetworkAPI, diagnostics utilities.\r\n- **Service Layer Separation**: Network calls executed solely within Python.\r\n- **Utility Modularity**: Logging helpers to be built as standalone functions.\r\n\r\n## Components and Interfaces\r\n\r\n### Component 1 – WebChannelBootstrap\r\n- **Purpose:** Ensure `QWebChannel` initialization, ping handshake, and retry logic without crashing Houdini.\r\n- **Interfaces:** `initialize_webchannel(web_view)`, returning diagnostics; exposes `logConsoleMessage` hook.\r\n- **Dependencies:** PySide6 `QWebEngineScript`, existing host bridge classes.\r\n- **Reuses:** Existing `create_window` workflow.\r\n\r\n### Component 2 – NetworkProxy\r\n- **Purpose:** Provide `openaiFetch`, `ollamaListModels`, and `ollamaPullModel` routes that map frontend requests to local Ollama endpoints.\r\n- **Interfaces:** Python slot methods returning JSON strings.\r\n- **Dependencies:** Python `urllib.request`, shared logging helper.\r\n- **Reuses:** Existing `NetworkAPI` class structure.\r\n\r\n### Component 3 – FrontendOllamaAdapter\r\n- **Purpose:** Frontend helper that calls new Qt slots, handles fallback, and logs outcomes.\r\n- **Interfaces:** Methods inside `OpenAIBaseClient` for Ollama branch.\r\n- **Dependencies:** `window.qt.network` objects, existing logger service.\r\n- **Reuses:** Current OpenAI client infrastructure.\r\n\r\n## Data Models\r\n\r\n### OllamaModel\r\n```\r\nOllamaModel = {\r\n  id: string,\r\n  object: 'model',\r\n  owned_by: 'ollama',\r\n  description?: string,\r\n  created_at?: string\r\n}\r\n```\r\n\r\n### NetworkProxyResponse\r\n```\r\nNetworkProxyResponse = {\r\n  status: number,\r\n  statusText?: string,\r\n  headers?: Record<string, string>,\r\n  body: string,\r\n  error?: string,\r\n  traceback?: string\r\n}\r\n```\r\n\r\n## Error Handling\r\n\r\n### Error Scenarios\r\n1. **Scenario 1:** WebChannel fails to initialize within retry limit.\r\n   - **Handling:** Emit warning logs, surface toast in frontend, keep Houdini running, provide manual retry button.\r\n   - **User Impact:** User is alerted that the bridge is unavailable; plugin remains open.\r\n\r\n2. **Scenario 2:** Ollama endpoint unreachable or returns error.\r\n   - **Handling:** Python proxy returns structured error JSON; frontend displays “Connection error” with details.\r\n   - **User Impact:** Model list shows fallback message; user can retry or adjust local service.\r\n\r\n## Testing Strategy\r\n\r\n### Unit Testing\r\n- Mock NetworkAPI calls to ensure proper JSON structure and error handling.\r\n- Validate Qt API exposure (ping returns `pong`).\r\n\r\n### Integration Testing\r\n- Launch Houdini plugin in test environment with fake Ollama server; verify model listing.\r\n- Simulate network failure to ensure graceful fallback.\r\n\r\n### End-to-End Testing\r\n- Full user flow: start Houdini plugin → ensure WebChannel ready → open model management → list/pull models.\r\n- Regression test: ensure existing Cherry Studio functionality (non-Ollama models) remains unaffected.\r\n",
  "fileStats": {
    "size": 5241,
    "lines": 118,
    "lastModified": "2025-09-26T13:57:18.977Z"
  },
  "comments": []
}